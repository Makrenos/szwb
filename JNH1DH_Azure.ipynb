{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credentials for the azure Ner service\n",
    "key = \"3c516844a8274d61b863123f938f9cea\"\n",
    "endpoint = \"https://szwb-azure.cognitiveservices.azure.com/\"\n",
    "\n",
    "#Imports\n",
    "import pandas as pd\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.ai.textanalytics import CategorizedEntity\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import re\n",
    "\n",
    "\n",
    "fields = ['Sentences','Word', 'POS', 'Tag']\n",
    "\n",
    "train = pd.read_csv(\"./data/TrainNER.csv\",sep=';',na_values='', encoding='latin1')\n",
    "train = train.rename(columns={train.columns[0]: 'Sentences', train.columns[1]: 'Word',train.columns[2]:  'POS',train.columns[3]:  'Tag'}) # adding first row\n",
    "del train[\"POS\"]\n",
    "train[\"Predicted\"]=\"\"\n",
    "\n",
    "\n",
    "test1 = pd.read_csv(\"./data/Test1NER.csv\",sep=';',na_values='', encoding='latin1')\n",
    "test2 = pd.read_csv(\"./data/Test2NER.csv\",sep=';',na_values='', encoding='latin1')\n",
    "\n",
    "#Add columns to test1 df\n",
    "test1.loc[-1] = [test1.columns[0], test1.columns[1], test1.columns[2]]  # rename columns\n",
    "test1.index = test1.index + 1  # shifting index\n",
    "test1 = test1.sort_index()  # sorting by index\n",
    "test1 = test1.rename(columns={test1.columns[0]: 'Sentences', test1.columns[1]: 'Word',test1.columns[2]:  'POS'}) # adding first row\n",
    "test1[\"Predicted\"]=\"\"\n",
    "\n",
    "#Add columns to test2 df\n",
    "test2.loc[-1] = [test2.columns[0], test2.columns[1]]  # rename columns\n",
    "test2.index = test2.index + 1  # shifting index\n",
    "test2 = test2.sort_index()  # sorting by index\n",
    "test2 = test2.rename(columns={test2.columns[0]: 'Sentences', test2.columns[1]: 'Word'}) # adding first row\n",
    "test2[\"Predicted\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Extract the sentences one-by-one from the tokenized csv file\n",
    "\n",
    "def getAllSentences(df):\n",
    "    all_sentences = list()\n",
    "    s = \"\"\n",
    "    for w in df.index:\n",
    "        if not pd.notna(df.iloc[w]['Sentences']):\n",
    "            if (not (('É' in str(df.iloc[w]['Word'])) or ('Ó' in str(df.iloc[w]['Word'])))):\n",
    "                s = s+str(df.iloc[w]['Word'])+\" \"\n",
    "        else:\n",
    "            s = s[:-1]\n",
    "            all_sentences.append(s)\n",
    "            s = \"\"\n",
    "            if (not (('É' in str(df.iloc[w]['Word'])) or ('Ó' in str(df.iloc[w]['Word'])))):\n",
    "                s = s+str(df.iloc[w]['Word'])+\" \"\n",
    "    s = s[:-1]\n",
    "    all_sentences.append(s)\n",
    "    all_sentences.pop(0)\n",
    "    return all_sentences    \n",
    "\n",
    "#getAllSentences(test1)\n",
    "#print(all_sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The azure NER client initalization and the entity recognition function.\n",
    "\n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "def entity_recognition(client,documents):\n",
    "    try:\n",
    "        result = client.recognize_entities(documents= documents)[0]\n",
    "        #The response contains compley entites, not word-by-word\n",
    "        return result\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Adds BIO tagging to the prediction.\n",
    "def bio_tagger(ne_tagged):\n",
    "\t\tbio_tagged = []\n",
    "\t\tprev_tag = \"O\"\n",
    "\t\tfor token, tag in ne_tagged:\n",
    "\t\t\tif tag == \"O\": #O\n",
    "\t\t\t\tbio_tagged.append((token, tag))\n",
    "\t\t\t\tprev_tag = tag\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif tag != \"O\" and prev_tag == \"O\": # Begin NE\n",
    "\t\t\t\tbio_tagged.append((token, \"B-\"+tag))\n",
    "\t\t\t\tprev_tag = tag\n",
    "\t\t\telif prev_tag != \"O\" and prev_tag == tag: # Inside NE\n",
    "\t\t\t\tbio_tagged.append((token, \"I-\"+tag))\n",
    "\t\t\t\tprev_tag = tag\n",
    "\t\t\telif prev_tag != \"O\" and prev_tag != tag: # Adjacent NE\n",
    "\t\t\t\tbio_tagged.append((token, \"B-\"+tag))\n",
    "\t\t\t\tprev_tag = tag\n",
    "\t\treturn bio_tagged\n",
    "\n",
    "#Calls the Azure Ner interface with the given test data\n",
    "#Returns the results in a list of tuples, what contains the (word,prediction) pairs\n",
    "def get_prediction(df):\n",
    "    allEntity = list()\n",
    "\n",
    "    submit = data_preparation(df)\n",
    "    \n",
    "    for i in range(len(submit)):\n",
    "\n",
    "        res = entity_recognition(client,submit[i:i+1])\n",
    "        l = len(res.entities)\n",
    "        filtered_res = list()\n",
    "        \n",
    "        for index, entity in enumerate(res.entities):\n",
    "            if index == 0:\n",
    "                filtered_res.append(entity)\n",
    "            if index > 0:\n",
    "                previous =  res.entities[index - 1]\n",
    "                if not  entity.text in previous.text:\n",
    "                    filtered_res.append(entity)\n",
    "        for entity in filtered_res:\n",
    "            if(len(entity.text.split(\" \"))>1):\n",
    "                s=entity.text.split(\" \")\n",
    "                for t in s:\n",
    "                    if t[0] == \".\":\n",
    "                        t = t.replace(\".\",\"\")\n",
    "                    tup = (t,str(entity.category)+'-'+str(entity.subcategory))\n",
    "                    allEntity.append(tup)\n",
    "            else:\n",
    "                if entity.text[0] == \".\":\n",
    "                        entity.text = entity.text.replace(\".\",\"\",1)\n",
    "                tup = (entity.text,str(entity.category)+'-'+str(entity.subcategory))\n",
    "                allEntity.append(tup)\n",
    "    formated_entity = list()\n",
    "    l = len(allEntity)\n",
    "    for index, entity in enumerate(allEntity):\n",
    "            if index == (l - 1):\n",
    "                formated_entity.append(entity)\n",
    "            if index < (l - 1):\n",
    "                next = allEntity[index + 1]\n",
    "                if not (entity[0] == next[0] and entity[1] == entity[1]):\n",
    "                    formated_entity.append(entity)\n",
    "    category_filtered_entity = list()\n",
    "    for w in formated_entity:\n",
    "        if \"Event\" in w[1]:\n",
    "            category_filtered_entity.append(w)\n",
    "        if \"Organization\" in w[1]:\n",
    "            category_filtered_entity.append(w)\n",
    "        if \"Location\" in w[1]:\n",
    "            category_filtered_entity.append(w)\n",
    "        if \"Person\" in w[1]:\n",
    "            category_filtered_entity.append(w)\n",
    "        if \"Time\" in w[1]:\n",
    "            category_filtered_entity.append(w)\n",
    "\n",
    "    shite_filtered_entity = list()\n",
    "    l = len(category_filtered_entity)\n",
    "    for index, entity in enumerate(category_filtered_entity):\n",
    "            if index == (l - 1):\n",
    "                shite_filtered_entity.append(entity)\n",
    "            if index < (l - 1):\n",
    "                next = category_filtered_entity[index + 1]\n",
    "                if index < (l - 2):\n",
    "                    next2 = category_filtered_entity[index + 2]\n",
    "                \n",
    "                if 'provinceOfficials' in entity[0]:\n",
    "                    temp1 = ('province',entity[1])\n",
    "                    temp2 = ('Officials',entity[1])\n",
    "                    shite_filtered_entity.append(temp1)\n",
    "                    shite_filtered_entity.append(temp2)\n",
    "                if 'BaseThe' in entity[0]:\n",
    "                    temp1 = ('Base',entity[1])\n",
    "                    temp2 = ('The',entity[1])\n",
    "                    shite_filtered_entity.append(temp1)\n",
    "                    shite_filtered_entity.append(temp2)\n",
    "                elif 'Shi' in entity[0] and 'ite' in next[0]:\n",
    "                    temp = (\"Shi'ite\",entity[1])\n",
    "                    shite_filtered_entity.append(temp)\n",
    "                elif (not ('ite' in entity[0])) and (not ('Independence' in entity[0] and 'holiday' in next2[0])) and (not ('Day' in entity[0] and 'holiday' in next[0])):\n",
    "                    shite_filtered_entity.append(entity)\n",
    "    return shite_filtered_entity\n",
    "\n",
    "#Segments up the list what contains the sentences\n",
    "#This is necessary, beacuse the azure NER has a char size limitation of 5120\n",
    "#The Azure NER has also a limitation of max 5 docs/request, and a 1MB request size.\n",
    "def data_preparation(df):\n",
    "    submit = list()\n",
    "    allSentences = getAllSentences(df)\n",
    "    temp=\"\"\n",
    "    for i in range(len(allSentences)):\n",
    "        temp = temp + allSentences[i]\n",
    "        temp = temp.replace(\"\\\\\",'')\n",
    "        if(len(temp) > 4750):\n",
    "            temp = temp.replace(\"\\\\\",'')\n",
    "            submit.append(temp)\n",
    "            temp=\"\"\n",
    "    submit.append(temp)\n",
    "    return submit\n",
    "\n",
    "#print(getPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regexes for the prediction filter, to shape them into the expected format\n",
    "b_event_regex = re.compile('(B-Event)+')\n",
    "i_event_regex = re.compile('(I-Event)+')\n",
    "b_org_regex = re.compile('(B-Organization)+')\n",
    "i_org_regex = re.compile('(I-Organization)+')\n",
    "b_person_regex = re.compile('(B-Person)+')\n",
    "i_person_regex = re.compile('(I-Person)+')\n",
    "b_time_regex = re.compile('B-.*(Time)+')\n",
    "i_time_regex = re.compile('I-.*(Time)+')\n",
    "\n",
    "#Insert the result of prediction into the given dataframe\n",
    "def insertPred(df,result):\n",
    "    i=0\n",
    "    for index, row in df.iterrows():  \n",
    "        if result[i][0] in row['Word']:\n",
    "            row['Predicted']=result[i][1]\n",
    "            if i < len(result)-1:\n",
    "                i = i+1\n",
    "        else:\n",
    "            row['Predicted']='O'\n",
    "\n",
    "def prediction_formatting(df):\n",
    "    for w in df.index:\n",
    "        if df['Predicted'][w]=='B-Location-GPE':\n",
    "            df['Predicted'][w]='B-gpe'\n",
    "        elif df['Predicted'][w]=='I-Location-GPE':\n",
    "            df['Predicted'][w]='I-gpe'\n",
    "\n",
    "        elif re.match(b_event_regex, df['Predicted'][w]):\n",
    "            df['Predicted'][w]='B-event'\n",
    "        elif re.match(i_event_regex, df['Predicted'][w]):\n",
    "            df['Predicted'][w]='I-event'\n",
    "\n",
    "        elif df['Predicted'][w]=='B-Location-Geographical':\n",
    "            df['Predicted'][w]='B-geo'\n",
    "        elif df['Predicted'][w]=='I-Location-Geographical':\n",
    "            df['Predicted'][w]='I-geo'\n",
    "\n",
    "        elif df['Predicted'][w]=='B-Location-Structural':\n",
    "            df['Predicted'][w]='B-obj'\n",
    "        elif df['Predicted'][w]=='I-Location-Structural':\n",
    "            df['Predicted'][w]='I-obj'\n",
    "\n",
    "        elif re.match(b_org_regex, df['Predicted'][w]):\n",
    "            df['Predicted'][w]='B-org'\n",
    "        elif re.match(i_org_regex, df['Predicted'][w]):\n",
    "            df['Predicted'][w]='I-org'\n",
    "\n",
    "        elif re.match(b_person_regex, df['Predicted'][w]):\n",
    "            df['Predicted'][w]='B-per'\n",
    "        elif re.match(i_person_regex, df['Predicted'][w]):\n",
    "            df['Predicted'][w]='I-per'\n",
    "\n",
    "        elif re.match(b_time_regex, df['Predicted'][w]):\n",
    "            df['Predicted'][w]='B-time'\n",
    "        elif re.match(i_time_regex, df['Predicted'][w]):\n",
    "            df['Predicted'][w]='I-time'\n",
    "        else:\n",
    "            df['Predicted'][w]='O'\n",
    "\n",
    "\n",
    "\n",
    "#Tagging all words\n",
    "def tag_all_words(df):\n",
    "    ne_tags = df[\"Predicted\"]\n",
    "    tokens =  df[\"Word\"]\n",
    "    tups = list()\n",
    "    for f,s in zip(ne_tags,tokens):\n",
    "        tups.append((s,f))\n",
    "    tokens,ne_tags = zip(*bio_tagger(tups))\n",
    "    df[\"Predicted\"] = ne_tags\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Command sequence\n",
    "insertPred(test1,get_prediction(test1))\n",
    "tag_all_words(test1)\n",
    "prediction_formatting(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertPred(test2,get_prediction(test2))\n",
    "tag_all_words(test2)\n",
    "prediction_formatting(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.to_csv(\"Submission1_Azure.csv\",index=False,sep = ';',encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.to_csv(\"Submission2_Azure.csv\",index=False,sep = ';',encoding='ANSI')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitbaseconda82c2496b1b4c44a38a11adee2c490de9",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
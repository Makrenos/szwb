{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "cannot import name 'TextAnalyticsApiVersion' from 'azure.ai.textanalytics' (D:\\anaconda3\\lib\\site-packages\\azure\\ai\\textanalytics\\__init__.py)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-dca9d1677556>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mazure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextanalytics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextAnalyticsClient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTextAnalyticsApiVersion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mazure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextanalytics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCategorizedEntity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mazure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextanalytics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TextAnalyticsApiVersion' from 'azure.ai.textanalytics' (D:\\anaconda3\\lib\\site-packages\\azure\\ai\\textanalytics\\__init__.py)"
     ]
    }
   ],
   "source": [
    "#Credentials for the azure Ner service\n",
    "key = \"3c516844a8274d61b863123f938f9cea\"\n",
    "endpoint = \"https://szwb-azure.cognitiveservices.azure.com/\"\n",
    "\n",
    "#Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "from azure.ai.textanalytics import TextAnalyticsClient, TextAnalyticsApiVersion\n",
    "from azure.ai.textanalytics import CategorizedEntity \n",
    "from azure.ai.textanalytics import _version\n",
    "from azure.ai.textanalytics import VERSION\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import math\n",
    "import re\n",
    "\n",
    "\n",
    "fields = ['Sentences','Word', 'POS', 'Tag']\n",
    "\n",
    "train = pd.read_csv(\"./data/TrainNER.csv\",sep=';',na_values='', encoding='latin1')\n",
    "train = train.rename(columns={train.columns[0]: 'Sentences', train.columns[1]: 'Word',train.columns[2]:  'POS',train.columns[3]:  'Tag'}) # adding first row\n",
    "del train[\"POS\"]\n",
    "train[\"Predicted\"]=\"\"\n",
    "\n",
    "\n",
    "test1 = pd.read_csv(\"./data/Test1NER.csv\",sep=';',na_values='', encoding='latin1')\n",
    "test2 = pd.read_csv(\"./data/Test2NER.csv\",sep=';',na_values='', encoding='latin1')\n",
    "\n",
    "#Add columns to test1 df\n",
    "test1.loc[-1] = [test1.columns[0], test1.columns[1], test1.columns[2]]  # rename columns\n",
    "test1.index = test1.index + 1  # shifting index\n",
    "test1 = test1.sort_index()  # sorting by index\n",
    "test1 = test1.rename(columns={test1.columns[0]: 'Sentences', test1.columns[1]: 'Word',test1.columns[2]:  'POS'}) # adding first row\n",
    "test1[\"Predicted\"]=\"\"\n",
    "\n",
    "#Add columns to test2 df\n",
    "test2.loc[-1] = [test2.columns[0], test2.columns[1]]  # rename columns\n",
    "test2.index = test2.index + 1  # shifting index\n",
    "test2 = test2.sort_index()  # sorting by index\n",
    "test2 = test2.rename(columns={test2.columns[0]: 'Sentences', test2.columns[1]: 'Word'}) # adding first row\n",
    "test2[\"Predicted\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            Sentences       Word  POS Predicted\n",
       "0      Sentence: 9001         In   IN          \n",
       "1                 NaN       2005   CD          \n",
       "2                 NaN          ,    ,          \n",
       "3                 NaN     Zambia  NNP          \n",
       "4                 NaN  qualified  VBD          \n",
       "...               ...        ...  ...       ...\n",
       "19397             NaN     Garang  NNP          \n",
       "19398             NaN        has  VBZ          \n",
       "19399             NaN  surpassed  VBN          \n",
       "19400             NaN        100   CD          \n",
       "19401             NaN          .    .          \n",
       "\n",
       "[19402 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentences</th>\n      <th>Word</th>\n      <th>POS</th>\n      <th>Predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sentence: 9001</td>\n      <td>In</td>\n      <td>IN</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>2005</td>\n      <td>CD</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>,</td>\n      <td>,</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>Zambia</td>\n      <td>NNP</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>qualified</td>\n      <td>VBD</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19397</th>\n      <td>NaN</td>\n      <td>Garang</td>\n      <td>NNP</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>19398</th>\n      <td>NaN</td>\n      <td>has</td>\n      <td>VBZ</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>19399</th>\n      <td>NaN</td>\n      <td>surpassed</td>\n      <td>VBN</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>19400</th>\n      <td>NaN</td>\n      <td>100</td>\n      <td>CD</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>19401</th>\n      <td>NaN</td>\n      <td>.</td>\n      <td>.</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n<p>19402 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Extract the sentences one-by-one from the tokenized csv file\n",
    "\n",
    "def getAllSentences(df):\n",
    "    all_sentences = list()\n",
    "    s = \"\"\n",
    "    for w in df.index:\n",
    "        if not pd.notna(df.iloc[w]['Sentences']):\n",
    "            if (not (('É' in str(df.iloc[w]['Word'])) or ('Ó' in str(df.iloc[w]['Word'])))):\n",
    "                s = s+str(df.iloc[w]['Word'])+\" \"\n",
    "        else:\n",
    "            s = s[:-1]\n",
    "            all_sentences.append(s)\n",
    "            s = \"\"\n",
    "            if (not (('É' in str(df.iloc[w]['Word'])) or ('Ó' in str(df.iloc[w]['Word'])))):\n",
    "                s = s+str(df.iloc[w]['Word'])+\" \"\n",
    "    s = s[:-1]\n",
    "    all_sentences.append(s)\n",
    "    all_sentences.pop(0)\n",
    "    return all_sentences    \n",
    "\n",
    "#getAllSentences(test1)\n",
    "#print(all_sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'TextAnalyticsApiVersion' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-c77fbab16081>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext_analytics_client\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mclient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauthenticate_client\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mentity_recognition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-c77fbab16081>\u001b[0m in \u001b[0;36mauthenticate_client\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mcredential\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mta_credential\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             api_version=TextAnalyticsApiVersion.V3_0)\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext_analytics_client\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TextAnalyticsApiVersion' is not defined"
     ]
    }
   ],
   "source": [
    "#The azure NER client initalization and the entity recognition function.\n",
    "\n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=endpoint, \n",
    "            credential=ta_credential,\n",
    "            api_version=TextAnalyticsApiVersion.V3_0)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "\n",
    "def entity_recognition(client,documents):\n",
    "    try:\n",
    "        result = client.recognize_entities(documents= documents)[0]\n",
    "        #The response contains compley entites, not word-by-word\n",
    "        return result\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Adds BIO tagging to the prediction.\n",
    "def bio_tagger(ne_tagged):\n",
    "\t\tbio_tagged = []\n",
    "\t\tprev_tag = \"O\"\n",
    "\t\tfor token, tag in ne_tagged:\n",
    "\t\t\tif tag == \"O\": #O\n",
    "\t\t\t\tbio_tagged.append((token, tag))\n",
    "\t\t\t\tprev_tag = tag\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif tag != \"O\" and prev_tag == \"O\": # Begin NE\n",
    "\t\t\t\tbio_tagged.append((token, \"B-\"+tag))\n",
    "\t\t\t\tprev_tag = tag\n",
    "\t\t\telif prev_tag != \"O\" and prev_tag == tag: # Inside NE\n",
    "\t\t\t\tbio_tagged.append((token, \"I-\"+tag))\n",
    "\t\t\t\tprev_tag = tag\n",
    "\t\t\telif prev_tag != \"O\" and prev_tag != tag: # Adjacent NE\n",
    "\t\t\t\tbio_tagged.append((token, \"B-\"+tag))\n",
    "\t\t\t\tprev_tag = tag\n",
    "\t\treturn bio_tagged\n",
    "\n",
    "#Calls the Azure Ner interface with the given test data\n",
    "#Returns the results in a list of tuples, what contains the (word,prediction) pairs\n",
    "def get_prediction(df):\n",
    "    allEntity = list()\n",
    "\n",
    "    submit = data_preparation(df)\n",
    "    \n",
    "    for i in range(len(submit)):\n",
    "        \n",
    "        res = entity_recognition(client,submit[i:i+1])\n",
    "       \n",
    "        l = len(res.entities)\n",
    "        filtered_res = list()\n",
    "\n",
    "        for index, entity in enumerate(res.entities):\n",
    "            if index == 0:\n",
    "                filtered_res.append(entity)\n",
    "            if index > 0:\n",
    "                previous =  res.entities[index - 1]\n",
    "                if not  entity.text in previous.text:\n",
    "                    filtered_res.append(entity)\n",
    "        for entity in filtered_res:\n",
    "            if(len(entity.text.split(\" \"))>1):\n",
    "                s=entity.text.split(\" \")\n",
    "                for t in s:\n",
    "                    if t[0] == \".\":\n",
    "                        t = t.replace(\".\",\"\")\n",
    "                    tup = (t,str(entity.category)+'-'+str(entity.subcategory))\n",
    "                    allEntity.append(tup)\n",
    "            else:\n",
    "                if entity.text[0] == \".\":\n",
    "                        entity.text = entity.text.replace(\".\",\"\",1)\n",
    "                tup = (entity.text,str(entity.category)+'-'+str(entity.subcategory))\n",
    "                allEntity.append(tup)\n",
    "    formated_entity = list()\n",
    "    l = len(allEntity)\n",
    "    for index, entity in enumerate(allEntity):\n",
    "            if index == (l - 1):\n",
    "                formated_entity.append(entity)\n",
    "            if index < (l - 1):\n",
    "                next = allEntity[index + 1]\n",
    "                if not (entity[0] == next[0] and entity[1] == entity[1]):\n",
    "                    formated_entity.append(entity)\n",
    "    category_filtered_entity = list()\n",
    "    for w in formated_entity:\n",
    "        if \"Event\" in w[1]:\n",
    "            category_filtered_entity.append(w)\n",
    "        if \"Organization\" in w[1]:\n",
    "            category_filtered_entity.append(w)\n",
    "        if \"Location\" in w[1]:\n",
    "            category_filtered_entity.append(w)\n",
    "        if \"Person\" in w[1]:\n",
    "            category_filtered_entity.append(w)\n",
    "        if \"Time\" in w[1]:\n",
    "            category_filtered_entity.append(w)\n",
    "\n",
    "    shite_filtered_entity = list()\n",
    "    l = len(category_filtered_entity)\n",
    "    for index, entity in enumerate(category_filtered_entity):\n",
    "            if index == (l - 1):\n",
    "                shite_filtered_entity.append(entity)\n",
    "            if index < (l - 1):\n",
    "                next = category_filtered_entity[index + 1]\n",
    "                if index < (l - 2):\n",
    "                    next2 = category_filtered_entity[index + 2]\n",
    "                \n",
    "                if 'provinceOfficials' in entity[0]:\n",
    "                    temp1 = ('province',entity[1])\n",
    "                    temp2 = ('Officials',entity[1])\n",
    "                    shite_filtered_entity.append(temp1)\n",
    "                    shite_filtered_entity.append(temp2)\n",
    "                if 'BaseThe' in entity[0]:\n",
    "                    temp1 = ('Base',entity[1])\n",
    "                    temp2 = ('The',entity[1])\n",
    "                    shite_filtered_entity.append(temp1)\n",
    "                    shite_filtered_entity.append(temp2)\n",
    "                elif 'Shi' in entity[0] and 'ite' in next[0]:\n",
    "                    temp = (\"Shi'ite\",entity[1])\n",
    "                    shite_filtered_entity.append(temp)\n",
    "                elif (not ('ite' in entity[0])) and (not ('Independence' in entity[0] and 'holiday' in next2[0])) and (not ('Day' in entity[0] and 'holiday' in next[0])):\n",
    "                    shite_filtered_entity.append(entity)\n",
    "    return shite_filtered_entity\n",
    "\n",
    "#Segments up the list what contains the sentences\n",
    "#This is necessary, beacuse the azure NER has a char size limitation of 5120\n",
    "#The Azure NER has also a limitation of max 5 docs/request, and a 1MB request size.\n",
    "def data_preparation(df):\n",
    "    submit = list()\n",
    "    allSentences = getAllSentences(df)\n",
    "    temp=\"\"\n",
    "    for i in range(len(allSentences)):\n",
    "        temp = temp + allSentences[i]\n",
    "        temp = temp.replace(\"\\\\\",'')\n",
    "        if(len(temp) > 4750):\n",
    "            temp = temp.replace(\"\\\\\",'')\n",
    "            submit.append(temp)\n",
    "            temp=\"\"\n",
    "    submit.append(temp)\n",
    "    return submit\n",
    "\n",
    "#print(getPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regexes for the prediction filter, to shape them into the expected format\n",
    "b_event_regex = re.compile('(B-Event)+')\n",
    "i_event_regex = re.compile('(I-Event)+')\n",
    "b_org_regex = re.compile('(B-Organization)+')\n",
    "i_org_regex = re.compile('(I-Organization)+')\n",
    "b_person_regex = re.compile('(B-Person)+')\n",
    "i_person_regex = re.compile('(I-Person)+')\n",
    "b_time_regex = re.compile('B-.*(Time)+')\n",
    "i_time_regex = re.compile('I-.*(Time)+')\n",
    "\n",
    "#Insert the result of prediction into the given dataframe\n",
    "def insertPred(df,result):\n",
    "    i=0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        if result[i][0] in row['Word']:\n",
    "            \n",
    "            row['Predicted']=result[i][1]\n",
    "            if i < len(res)-1:\n",
    "                i = i+1\n",
    "        else:\n",
    "            \n",
    "            row['Predicted']='O'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prediction_formatting(df):\n",
    "    for w in df.index:\n",
    "        if df['Predicted'][w]=='B-Location-GPE':\n",
    "            df['Predicted'][w]='B-gpe'\n",
    "        elif df['Predicted'][w]=='I-Location-GPE':\n",
    "            df['Predicted'][w]='I-gpe'\n",
    "\n",
    "        elif re.match(b_event_regex, df['Predicted'][w]):\n",
    "            df['Predicted'][w]='B-event'\n",
    "        elif re.match(i_event_regex, df['Predicted'][w]):\n",
    "            df['Predicted'][w]='I-event'\n",
    "\n",
    "        elif df['Predicted'][w]=='B-Location-Geographical':\n",
    "            df['Predicted'][w]='B-geo'\n",
    "        elif df['Predicted'][w]=='I-Location-Geographical':\n",
    "            df['Predicted'][w]='I-geo'\n",
    "\n",
    "        elif df['Predicted'][w]=='B-Location-Structural':\n",
    "            df['Predicted'][w]='B-obj'\n",
    "        elif df['Predicted'][w]=='I-Location-Structural':\n",
    "            df['Predicted'][w]='I-obj'\n",
    "\n",
    "        elif re.match(b_org_regex, df['Predicted'][w]):\n",
    "            df['Predicted'][w]='B-org'\n",
    "        elif re.match(i_org_regex, df['Predicted'][w]):\n",
    "            df['Predicted'][w]='I-org'\n",
    "\n",
    "        elif re.match(b_person_regex, df['Predicted'][w]):\n",
    "            df['Predicted'][w]='B-per'\n",
    "        elif re.match(i_person_regex, df['Predicted'][w]):\n",
    "            df['Predicted'][w]='I-per'\n",
    "\n",
    "        elif re.match(b_time_regex, df['Predicted'][w]):\n",
    "            df['Predicted'][w]='B-time'\n",
    "        elif re.match(i_time_regex, df['Predicted'][w]):\n",
    "            df['Predicted'][w]='I-time'\n",
    "        else:\n",
    "            df['Predicted'][w]='O'\n",
    "\n",
    "\n",
    "\n",
    "#Tagging all words\n",
    "def tag_all_words(df):\n",
    "    ne_tags = df[\"Predicted\"]\n",
    "    tokens =  df[\"Word\"]\n",
    "    tups = list()\n",
    "    for f,s in zip(ne_tags,tokens):\n",
    "        tups.append((s,f))\n",
    "    tokens,ne_tags = zip(*bio_tagger(tups))\n",
    "    df[\"Predicted\"] = ne_tags\n",
    "\n",
    "\n",
    "#Command seque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"President Karzai thanked his allies for their help in battling terrorism .The commander of NATO 's Afghan force , British General David Richards , said the unity of command the transfer brought will enhance the effectiveness of the overall operation .\"]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "data_preparation(train[0:42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RecognizeEntitiesResult(id=0, entities=[CategorizedEntity(text=Karzai, category=Person, subcategory=None, confidence_score=0.82), CategorizedEntity(text=allies, category=PersonType, subcategory=None, confidence_score=0.44), CategorizedEntity(text=commander, category=PersonType, subcategory=None, confidence_score=0.75), CategorizedEntity(text=NATO, category=Organization, subcategory=None, confidence_score=0.65), CategorizedEntity(text=David Richards, category=Person, subcategory=None, confidence_score=0.65)], warnings=[], statistics=None, is_error=False)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "entity_recognition(client,data_preparation(train[0:42]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Karzai', 'Person-None'),\n",
       " ('allies', 'PersonType-None'),\n",
       " ('commander', 'PersonType-None'),\n",
       " ('NATO', 'Organization-None'),\n",
       " ('David', 'Person-None'),\n",
       " ('Richards', 'Person-None')]"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "get_prediction(train[0:43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "string index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ca738ec5e456>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minsertPred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mget_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtag_all_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprediction_formatting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-7799bd300773>\u001b[0m in \u001b[0;36mget_prediction\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\".\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                     \u001b[0mtup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubcategory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "insertPred(train,get_prediction(train))\n",
    "tag_all_words(train)\n",
    "prediction_formatting(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'Predicted'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Predicted'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-283-8e41a7d0f650>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0minsertPred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mget_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtag_all_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprediction_formatting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-269-f7a24dc89b5a>\u001b[0m in \u001b[0;36mtag_all_words\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;31m#Tagging all words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtag_all_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mne_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Predicted\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Word\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mtups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Predicted'"
     ]
    }
   ],
   "source": [
    "insertPred(test1,get_prediction(test1))\n",
    "tag_all_words(test1)\n",
    "prediction_formatting(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertPred(test2,get_prediction(test2))\n",
    "tag_all_words(test2)\n",
    "prediction_formatting(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           Sentences           Word Predicted\n",
       "0     Sentence: 9901            The         O\n",
       "1                NaN  International     B-org\n",
       "2                NaN      Committee     I-org\n",
       "3                NaN             of     I-org\n",
       "4                NaN            the     I-org\n",
       "...              ...            ...       ...\n",
       "9873             NaN            the         O\n",
       "9874             NaN           U.S.     B-org\n",
       "9875             NaN         Census     I-org\n",
       "9876             NaN         Bureau     I-org\n",
       "9877             NaN              .         O\n",
       "\n",
       "[9878 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentences</th>\n      <th>Word</th>\n      <th>Predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sentence: 9901</td>\n      <td>The</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>International</td>\n      <td>B-org</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>Committee</td>\n      <td>I-org</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>of</td>\n      <td>I-org</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>the</td>\n      <td>I-org</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9873</th>\n      <td>NaN</td>\n      <td>the</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>9874</th>\n      <td>NaN</td>\n      <td>U.S.</td>\n      <td>B-org</td>\n    </tr>\n    <tr>\n      <th>9875</th>\n      <td>NaN</td>\n      <td>Census</td>\n      <td>I-org</td>\n    </tr>\n    <tr>\n      <th>9876</th>\n      <td>NaN</td>\n      <td>Bureau</td>\n      <td>I-org</td>\n    </tr>\n    <tr>\n      <th>9877</th>\n      <td>NaN</td>\n      <td>.</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n<p>9878 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 297
    }
   ],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.to_csv(\"Submission_1.csv\",index=False,sep = ';',encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.to_csv(\"Submission_2.csv\",index=False,sep = ';',encoding='ANSI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train_pred.csv\",index=False,sep = ';',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            Sentences       Word  Tag new Predicted\n",
       "0      Sentence: 9001         In   IN   !         O\n",
       "1                 NaN       2005   CD   !    B-time\n",
       "2                 NaN          ,    ,   !         O\n",
       "3                 NaN     Zambia  NNP   !     B-gpe\n",
       "4                 NaN  qualified  VBD   !         O\n",
       "...               ...        ...  ...  ..       ...\n",
       "19397             NaN     Garang  NNP   !         O\n",
       "19398             NaN        has  VBZ   !         O\n",
       "19399             NaN  surpassed  VBN   !         O\n",
       "19400             NaN        100   CD   !         O\n",
       "19401             NaN          .    .   !         O\n",
       "\n",
       "[19402 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentences</th>\n      <th>Word</th>\n      <th>Tag</th>\n      <th>new</th>\n      <th>Predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sentence: 9001</td>\n      <td>In</td>\n      <td>IN</td>\n      <td>!</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>2005</td>\n      <td>CD</td>\n      <td>!</td>\n      <td>B-time</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>,</td>\n      <td>,</td>\n      <td>!</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>Zambia</td>\n      <td>NNP</td>\n      <td>!</td>\n      <td>B-gpe</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>qualified</td>\n      <td>VBD</td>\n      <td>!</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19397</th>\n      <td>NaN</td>\n      <td>Garang</td>\n      <td>NNP</td>\n      <td>!</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>19398</th>\n      <td>NaN</td>\n      <td>has</td>\n      <td>VBZ</td>\n      <td>!</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>19399</th>\n      <td>NaN</td>\n      <td>surpassed</td>\n      <td>VBN</td>\n      <td>!</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>19400</th>\n      <td>NaN</td>\n      <td>100</td>\n      <td>CD</td>\n      <td>!</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>19401</th>\n      <td>NaN</td>\n      <td>.</td>\n      <td>.</td>\n      <td>!</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n<p>19402 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 331
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'train.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-369-621cfd117811>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'new'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m';'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#if(not(df[3][i]==\"O\" and df[\"Predicted\"][i]==\"O\")):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3203\u001b[0m         )\n\u001b[1;32m-> 3204\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m                 \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m             )\n\u001b[0;32m    190\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'train.csv'"
     ]
    }
   ],
   "source": [
    "all=0\n",
    "match = 0\n",
    "train['new'] = \"\"\n",
    "train.to_csv(\"train.csv\",index=False,sep = ';',encoding='utf-8')\n",
    "for i in range(len(train)):\n",
    "    #if(not(df[3][i]==\"O\" and df[\"Predicted\"][i]==\"O\")):\n",
    "    if (not (train[\"Tag\"][i] == \"O\" and train[\"Predicted\"][i] == \"O\")):\n",
    "        all=all+1\n",
    "        train['new'][i] = \"!\"\n",
    "        #if (df[3][i] == df[\"Predicted\"][i]):\n",
    "        if (train[\"Tag\"][i] == train[\"Predicted\"][i]):\n",
    "            match = match + 1\n",
    "print(\"Match: \",match)\n",
    "print(\"All:   \",all)\n",
    "print(\"Predicted: \",round((match/all)*100,4),\"%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitbaseconda82c2496b1b4c44a38a11adee2c490de9",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
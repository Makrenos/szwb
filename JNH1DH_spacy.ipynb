{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "from spacy.tokens.doc import Doc\n",
    "from spacy.gold import GoldParse\n",
    "import plac\n",
    "from spacy.gold import offsets_from_biluo_tags\n",
    "\n",
    "#load my model\n",
    "nlp = spacy.load(\"./model_new\")\n",
    "\n",
    "fields = ['Sentences','Word', 'POS', 'Tag']\n",
    "\n",
    "train = pd.read_csv(\"./data/TrainNER.csv\",sep=';',na_values='', encoding='latin1')\n",
    "train = train.rename(columns={train.columns[0]: 'Sentences', train.columns[1]: 'Word',train.columns[2]:  'POS',train.columns[3]:  'Tag'}) # adding first row\n",
    "del train[\"POS\"]\n",
    "train[\"Predicted\"]=\"\"\n",
    "\n",
    "\n",
    "test1 = pd.read_csv(\"./data/Test1NER.csv\",sep=';',na_values='', encoding='latin1')\n",
    "test2 = pd.read_csv(\"./data/Test2NER.csv\",sep=';',na_values='', encoding='latin1')\n",
    "\n",
    "#Add columns to test1 df\n",
    "test1.loc[-1] = [test1.columns[0], test1.columns[1], test1.columns[2]]  # rename columns\n",
    "test1.index = test1.index + 1  # shifting index\n",
    "test1 = test1.sort_index()  # sorting by index\n",
    "test1 = test1.rename(columns={test1.columns[0]: 'Sentences', test1.columns[1]: 'Word',test1.columns[2]:  'POS'}) # adding first row\n",
    "test1[\"Predicted\"]=\"\"\n",
    "\n",
    "#Add columns to test2 df\n",
    "test2.loc[-1] = [test2.columns[0], test2.columns[1]]  # rename columns\n",
    "test2.index = test2.index + 1  # shifting index\n",
    "test2 = test2.sort_index()  # sorting by index\n",
    "test2 = test2.rename(columns={test2.columns[0]: 'Sentences', test2.columns[1]: 'Word'}) # adding first row\n",
    "test2[\"Predicted\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model trainer method\n",
    "#source: https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py\n",
    "\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    ")\n",
    "def main(model=None, output_dir=None, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in train_data[0:8998]:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    # only train NER\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        # show warnings for misaligned entity spans once\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data[0:8998])\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(train_data[0:8998], size=compounding(4.0, 32.0, 1.001))\n",
    "            print(batches)\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.4,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in train_data[0:8998]:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in train_data[0:8998]:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates BILOU tagging from BIO tagging for spacy.\n",
    "\n",
    "def bio_to_bilou(tags):\n",
    "    result = list()\n",
    "    if(tags[0][0] == 'B' and tags[1][0] == 'O'):\n",
    "        result.append(tags[0][:0] + 'U' + tags[0][0 +1:])\n",
    "    else:\n",
    "        result.append(tags[0])\n",
    "    for i in range(1,len(tags)-1):\n",
    "        \n",
    "        current_iob = tags[i][0]\n",
    "        prev_iob = tags[i-1][0]\n",
    "        next_iob = tags[i+1][0]\n",
    "        # Outside entities\n",
    "\n",
    "        \n",
    "        if current_iob == 'O':\n",
    "            result.append(tags[i])\n",
    "        elif (prev_iob == 'O' and next_iob == 'O' and current_iob != 'O') or (prev_iob == 'I' and current_iob == 'B' and next_iob == 'O') or (prev_iob == 'B' and current_iob == 'B' and next_iob == 'O') or (prev_iob == 'B' and current_iob == 'B' and next_iob == 'B') or (prev_iob == 'O' and current_iob == 'B' and next_iob == 'B'):\n",
    "            result.append(tags[i][:0] + 'U' + tags[i][0 +1:])\n",
    "        # Unit length entities\n",
    "        elif (prev_iob == 'O' and next_iob != 'O') or (prev_iob == 'I' and next_iob != 'O') or (prev_iob == 'B' and next_iob != 'O'):\n",
    "            result.append(tags[i][:0] + 'B' + tags[i][0 +1:])\n",
    "        elif (prev_iob != 'O' and next_iob == 'O') or (prev_iob != 'O' and next_iob == 'B'):\n",
    "            result.append(tags[i][:0] + 'L' + tags[i][0 +1:])\n",
    "        elif (prev_iob != 'O' and next_iob != 'O'):\n",
    "            result.append(tags[i][:0] + 'I' + tags[i][0 +1:])\n",
    "    return result\n",
    "\n",
    "#Adds BIO tagging to the prediction.\n",
    "def bio_tagger(ne_tagged):\n",
    "\t\tbio_tagged = []\n",
    "\t\tprev_tag = \"O\"\n",
    "\t\tfor token, tag in ne_tagged:\n",
    "\t\t\tif tag == \"O\": #O\n",
    "\t\t\t\tbio_tagged.append((token, tag))\n",
    "\t\t\t\tprev_tag = tag\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif tag != \"O\" and prev_tag == \"O\": # Begin NE\n",
    "\t\t\t\tbio_tagged.append((token, \"B-\"+tag))\n",
    "\t\t\t\tprev_tag = tag\n",
    "\t\t\telif prev_tag != \"O\" and prev_tag == tag: # Inside NE\n",
    "\t\t\t\tbio_tagged.append((token, \"I-\"+tag))\n",
    "\t\t\t\tprev_tag = tag\n",
    "\t\t\telif prev_tag != \"O\" and prev_tag != tag: # Adjacent NE\n",
    "\t\t\t\tbio_tagged.append((token, \"B-\"+tag))\n",
    "\t\t\t\tprev_tag = tag\n",
    "\t\treturn bio_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper methods, to substract data from the dataframes\n",
    "\n",
    "def getAllTrainSentences(df):\n",
    "    all_sentences = list()\n",
    "    s = \"\"\n",
    "    for w in df.index:\n",
    "        if not pd.notna(df.iloc[w]['Sentences']):\n",
    "            if (not (('É' in str(df.iloc[w]['Word'])) or ('Ó' in str(df.iloc[w]['Word'])))):\n",
    "                s = s+str(df.iloc[w]['Word'])+\" \"\n",
    "        else:\n",
    "            s = s[:-1]\n",
    "            all_sentences.append(s)\n",
    "            s = \"\"\n",
    "            if (not (('É' in str(df.iloc[w]['Word'])) or ('Ó' in str(df.iloc[w]['Word'])))):\n",
    "                s = s+str(df.iloc[w]['Word'])+\" \"\n",
    "    s = s[:-5]\n",
    "    temp = list()\n",
    "    temp.append(s)\n",
    "    all_sentences.append(temp)\n",
    "    all_sentences.pop(0)\n",
    "    return all_sentences\n",
    "\n",
    "def getAllTags(df):\n",
    "    all_tags = list()\n",
    "    s = list()\n",
    "    for w in df.index:\n",
    "        if not pd.notna(df.iloc[w]['Sentences']):\n",
    "            if (not (('É' in str(df.iloc[w]['Word'])) or ('Ó' in str(df.iloc[w]['Word'])))):\n",
    "                s.append(df.iloc[w]['Tag'])\n",
    "        else:\n",
    "            all_tags.append(s)\n",
    "            s = list()\n",
    "            if (not (('É' in str(df.iloc[w]['Word'])) or ('Ó' in str(df.iloc[w]['Word'])))):\n",
    "                 s.append(df.iloc[w]['Tag'])\n",
    "    s.pop()\n",
    "    all_tags.append(s)\n",
    "    all_tags.pop(0)\n",
    "    return all_tags\n",
    "\n",
    "def getAllSentences(df):\n",
    "    all_sentences = list()\n",
    "    s = \"\"\n",
    "    for w in df.index:\n",
    "        if not pd.notna(df.iloc[w]['Sentences']):\n",
    "            if (not (('É' in str(df.iloc[w]['Word'])) or ('Ó' in str(df.iloc[w]['Word'])))):\n",
    "                s = s+str(df.iloc[w]['Word'])+\" \"\n",
    "        else:\n",
    "            s = s[:-1]\n",
    "            all_sentences.append(s)\n",
    "            s = \"\"\n",
    "            if (not (('É' in str(df.iloc[w]['Word'])) or ('Ó' in str(df.iloc[w]['Word'])))):\n",
    "                s = s+str(df.iloc[w]['Word'])+\" \"\n",
    "    s = s[:-1]\n",
    "    all_sentences.append(s)\n",
    "    all_sentences.pop(0)\n",
    "    return all_sentences\n",
    "\n",
    "def getAllWords(df):\n",
    "    all_words = list()\n",
    "    for index, row in df.iterrows():\n",
    "        all_words.append(str(row['Word']))\n",
    "    return all_words\n",
    "\n",
    "def getAllEntities(df):\n",
    "    all_entities = list()\n",
    "    for index, row in df.iterrows():\n",
    "        all_entities.append(str(row['Tag']))\n",
    "    return all_entities\n",
    "\n",
    "#Insert the result of prediction into the given dataframe\n",
    "def insertPred(df,result):\n",
    "    i=0\n",
    "    for index, row in df.iterrows():\n",
    "        if result[i][0] in row['Word']:\n",
    "            row['Predicted']=result[i][1]\n",
    "            if i < len(result)-1:\n",
    "                i = i+1\n",
    "        else: \n",
    "            row['Predicted']='O'\n",
    "\n",
    "#Gets the entites from the test text by my trained model\n",
    "def get_all_entities(df):\n",
    "    df_all_sentences = getAllSentences(df)\n",
    "    all_entities = list()\n",
    "    for text in df_all_sentences:\n",
    "            doc = nlp(text)\n",
    "            all_entities.append(substract_entities(doc.ents))\n",
    "            #print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            #print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "    return all_entities\n",
    "\n",
    "#Tagging all words\n",
    "def tag_all_words(df):\n",
    "    ne_tags = df[\"Predicted\"]\n",
    "    tokens =  df[\"Word\"]\n",
    "    tups = list()\n",
    "    for f,s in zip(ne_tags,tokens):\n",
    "        tups.append((s,f))\n",
    "    tokens,ne_tags = zip(*bio_tagger(tups))\n",
    "    df[\"Predicted\"] = ne_tags\n",
    "    \n",
    "#helper methods for the entite substraction\n",
    "def substract_list(entities):\n",
    "    all_entities = list()\n",
    "    for elem in entities:\n",
    "        for i in range(len(elem)):\n",
    "            all_entities.append(elem[i])\n",
    "    return all_entities\n",
    "\n",
    "def substract_entities(doc):\n",
    "    allEntity = list()\n",
    "    for entity in doc:\n",
    "        if(len(entity.text.split(\" \"))>1):\n",
    "            s=entity.text.split(\" \")\n",
    "            for t in s:\n",
    "                tup = (t,str(entity.label_))\n",
    "                allEntity.append(tup)\n",
    "        else:\n",
    "            tup = (entity.text,str(entity.label_))\n",
    "            allEntity.append(tup)\n",
    "    return allEntity\n",
    "\n",
    "#More helper methods to preconsume data to be able to give it to the spacy training method\n",
    "def getAllTrainData():\n",
    "    train_data= list()\n",
    "    for i in range(len(all_sentences)):\n",
    "        train_data.append((all_sentences[i],{\"entities\": offsets_from_biluo_tags(nlp(str(all_sentences[i])), bio_to_bilou(all_tags[i]))}))\n",
    "    return train_data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The sequence to get the train data properly\n",
    "all_sentences = getAllTrainSentences(train)\n",
    "all_tags = getAllTags(train)\n",
    "train_data= getAllTrainData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertPred(test1,substract_list(get_all_entities(test1)))\n",
    "tag_all_words(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertPred(test2,substract_list(get_all_entities(test2)))\n",
    "tag_all_words(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.to_csv(\"Submission_trained_spacy_1.csv\",index=False,sep = ';',encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.to_csv(\"Submission_trained_spacy_2.csv\",index=False,sep = ';',encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitbaseconda82c2496b1b4c44a38a11adee2c490de9",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}